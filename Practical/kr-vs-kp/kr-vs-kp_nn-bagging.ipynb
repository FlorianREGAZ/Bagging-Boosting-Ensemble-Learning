{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kr-vs-kp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "\n",
    "categories = ['bkblk','bknwy','bkon8','bkona','bkspr','bkxbq','bkxcr','bkxwp','blxwp','bxqsq','cntxt','dsopp','dwipd','hdchk','katri','mulch','qxmsq','r2ar8','reskd','reskr','rimmx','rkxwp','rxmsq','simpl','skach','skewr','skrxp','spcop','stlmt','thrsk','wkcti','wkna8','wknck','wkovl','wkpos','wtoeg']\n",
    "\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "OH_df = pd.DataFrame(OH_encoder.fit_transform(df[categories]))\n",
    "\n",
    "enc_df = pd.concat([OH_df, df[\"class\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = enc_df.drop(\"class\", axis=1)\n",
    "test = enc_df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, test, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.14216653\n",
      "Iteration 2, loss = 0.30906904\n",
      "Iteration 3, loss = 0.15620338\n",
      "Iteration 4, loss = 0.11543704\n",
      "Iteration 5, loss = 0.08030149\n",
      "Iteration 6, loss = 0.07159124\n",
      "Iteration 7, loss = 0.07743882\n",
      "Iteration 8, loss = 0.06126738\n",
      "Iteration 9, loss = 0.04834072\n",
      "Iteration 10, loss = 0.04263645\n",
      "Iteration 11, loss = 0.03963293\n",
      "Iteration 12, loss = 0.04039949\n",
      "Iteration 13, loss = 0.03543922\n",
      "Iteration 14, loss = 0.02726571\n",
      "Iteration 15, loss = 0.03748782\n",
      "Iteration 16, loss = 0.06502317\n",
      "Iteration 17, loss = 0.05480651\n",
      "Iteration 18, loss = 0.04011286\n",
      "Iteration 19, loss = 0.02564983\n",
      "Iteration 20, loss = 0.01727032\n",
      "Iteration 21, loss = 0.01537764\n",
      "Iteration 22, loss = 0.01234271\n",
      "Iteration 23, loss = 0.01309119\n",
      "Iteration 24, loss = 0.01276728\n",
      "Iteration 25, loss = 0.01074184\n",
      "Iteration 26, loss = 0.00890489\n",
      "Iteration 27, loss = 0.00748257\n",
      "Iteration 28, loss = 0.00717161\n",
      "Iteration 29, loss = 0.01589381\n",
      "Iteration 30, loss = 0.14822786\n",
      "Iteration 31, loss = 0.06370636\n",
      "Iteration 32, loss = 0.01882013\n",
      "Iteration 33, loss = 0.01246353\n",
      "Iteration 34, loss = 0.00718247\n",
      "Iteration 35, loss = 0.00573788\n",
      "Iteration 36, loss = 0.00470977\n",
      "Iteration 37, loss = 0.00520074\n",
      "Iteration 38, loss = 0.00641337\n",
      "Iteration 39, loss = 0.00462324\n",
      "Iteration 40, loss = 0.00553175\n",
      "Iteration 41, loss = 0.00558872\n",
      "Iteration 42, loss = 0.00547614\n",
      "Iteration 43, loss = 0.00312150\n",
      "Iteration 44, loss = 0.00282051\n",
      "Iteration 45, loss = 0.00262585\n",
      "Iteration 46, loss = 0.00279692\n",
      "Iteration 47, loss = 0.00216987\n",
      "Iteration 48, loss = 0.00207700\n",
      "Iteration 49, loss = 0.00239649\n",
      "Iteration 50, loss = 0.00233766\n",
      "Iteration 51, loss = 0.00210362\n",
      "Iteration 52, loss = 0.00180767\n",
      "Iteration 53, loss = 0.00177127\n",
      "Iteration 54, loss = 0.00206368\n",
      "Iteration 55, loss = 0.00200894\n",
      "Iteration 56, loss = 0.00189570\n",
      "Iteration 57, loss = 0.00224646\n",
      "Iteration 58, loss = 0.00312221\n",
      "Iteration 59, loss = 0.00367952\n",
      "Iteration 60, loss = 0.00214050\n",
      "Iteration 61, loss = 0.00289246\n",
      "Iteration 62, loss = 0.00230438\n",
      "Iteration 63, loss = 0.00324256\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.62016375\n",
      "Iteration 2, loss = 0.48879108\n",
      "Iteration 3, loss = 0.23710544\n",
      "Iteration 4, loss = 0.14550350\n",
      "Iteration 5, loss = 0.10652619\n",
      "Iteration 6, loss = 0.08180145\n",
      "Iteration 7, loss = 0.07388860\n",
      "Iteration 8, loss = 0.07151137\n",
      "Iteration 9, loss = 0.07269654\n",
      "Iteration 10, loss = 0.05936484\n",
      "Iteration 11, loss = 0.05739957\n",
      "Iteration 12, loss = 0.05653525\n",
      "Iteration 13, loss = 0.05290483\n",
      "Iteration 14, loss = 0.04466235\n",
      "Iteration 15, loss = 0.03943997\n",
      "Iteration 16, loss = 0.03788226\n",
      "Iteration 17, loss = 0.02761572\n",
      "Iteration 18, loss = 0.03021957\n",
      "Iteration 19, loss = 0.02389027\n",
      "Iteration 20, loss = 0.02052334\n",
      "Iteration 21, loss = 0.01268312\n",
      "Iteration 22, loss = 0.00987102\n",
      "Iteration 23, loss = 0.00790254\n",
      "Iteration 24, loss = 0.01126200\n",
      "Iteration 25, loss = 0.01301783\n",
      "Iteration 26, loss = 0.01516491\n",
      "Iteration 27, loss = 0.00997590\n",
      "Iteration 28, loss = 0.01105517\n",
      "Iteration 29, loss = 0.00956645\n",
      "Iteration 30, loss = 0.00763199\n",
      "Iteration 31, loss = 0.00611434\n",
      "Iteration 32, loss = 0.00512047\n",
      "Iteration 33, loss = 0.01325241\n",
      "Iteration 34, loss = 0.00788128\n",
      "Iteration 35, loss = 0.00434625\n",
      "Iteration 36, loss = 0.00676240\n",
      "Iteration 37, loss = 0.00947686\n",
      "Iteration 38, loss = 0.00775194\n",
      "Iteration 39, loss = 0.00917446\n",
      "Iteration 40, loss = 0.01373224\n",
      "Iteration 41, loss = 0.00860011\n",
      "Iteration 42, loss = 0.00735338\n",
      "Iteration 43, loss = 0.00384686\n",
      "Iteration 44, loss = 0.00376192\n",
      "Iteration 45, loss = 0.00270772\n",
      "Iteration 46, loss = 0.00157951\n",
      "Iteration 47, loss = 0.00147152\n",
      "Iteration 48, loss = 0.00142954\n",
      "Iteration 49, loss = 0.00171463\n",
      "Iteration 50, loss = 0.00109622\n",
      "Iteration 51, loss = 0.00110788\n",
      "Iteration 52, loss = 0.00107777\n",
      "Iteration 53, loss = 0.00097881\n",
      "Iteration 54, loss = 0.00098110\n",
      "Iteration 55, loss = 0.00111296\n",
      "Iteration 56, loss = 0.00099932\n",
      "Iteration 57, loss = 0.00138827\n",
      "Iteration 58, loss = 0.00149323\n",
      "Iteration 59, loss = 0.00112509\n",
      "Iteration 60, loss = 0.00112858\n",
      "Iteration 61, loss = 0.00136343\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09289089\n",
      "Iteration 2, loss = 0.39589197\n",
      "Iteration 3, loss = 0.17565613\n",
      "Iteration 4, loss = 0.12545984\n",
      "Iteration 5, loss = 0.10722655\n",
      "Iteration 6, loss = 0.09594155\n",
      "Iteration 7, loss = 0.09995331\n",
      "Iteration 8, loss = 0.10791321\n",
      "Iteration 9, loss = 0.09804483\n",
      "Iteration 10, loss = 0.13807030\n",
      "Iteration 11, loss = 0.13206894\n",
      "Iteration 12, loss = 0.08486588\n",
      "Iteration 13, loss = 0.06961035\n",
      "Iteration 14, loss = 0.05813194\n",
      "Iteration 15, loss = 0.04398412\n",
      "Iteration 16, loss = 0.04068807\n",
      "Iteration 17, loss = 0.03137774\n",
      "Iteration 18, loss = 0.02955971\n",
      "Iteration 19, loss = 0.02842365\n",
      "Iteration 20, loss = 0.02698387\n",
      "Iteration 21, loss = 0.02615975\n",
      "Iteration 22, loss = 0.02238750\n",
      "Iteration 23, loss = 0.02160073\n",
      "Iteration 24, loss = 0.02335874\n",
      "Iteration 25, loss = 0.01303814\n",
      "Iteration 26, loss = 0.01700178\n",
      "Iteration 27, loss = 0.02099655\n",
      "Iteration 28, loss = 0.05244856\n",
      "Iteration 29, loss = 0.02570679\n",
      "Iteration 30, loss = 0.02205616\n",
      "Iteration 31, loss = 0.03803023\n",
      "Iteration 32, loss = 0.04166961\n",
      "Iteration 33, loss = 0.03078275\n",
      "Iteration 34, loss = 0.01588926\n",
      "Iteration 35, loss = 0.01020316\n",
      "Iteration 36, loss = 0.01132417\n",
      "Iteration 37, loss = 0.01144513\n",
      "Iteration 38, loss = 0.02542738\n",
      "Iteration 39, loss = 0.02404263\n",
      "Iteration 40, loss = 0.01668329\n",
      "Iteration 41, loss = 0.01920641\n",
      "Iteration 42, loss = 0.01361335\n",
      "Iteration 43, loss = 0.01006589\n",
      "Iteration 44, loss = 0.00724621\n",
      "Iteration 45, loss = 0.01058502\n",
      "Iteration 46, loss = 0.00797499\n",
      "Iteration 47, loss = 0.00861404\n",
      "Iteration 48, loss = 0.00756339\n",
      "Iteration 49, loss = 0.00904835\n",
      "Iteration 50, loss = 0.01381067\n",
      "Iteration 51, loss = 0.01382215\n",
      "Iteration 52, loss = 0.03238144\n",
      "Iteration 53, loss = 0.02624827\n",
      "Iteration 54, loss = 0.03719344\n",
      "Iteration 55, loss = 0.02209685\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17455277\n",
      "Iteration 2, loss = 0.72469168\n",
      "Iteration 3, loss = 0.69227270\n",
      "Iteration 4, loss = 0.69246077\n",
      "Iteration 5, loss = 0.69236404\n",
      "Iteration 6, loss = 0.69277488\n",
      "Iteration 7, loss = 0.69280397\n",
      "Iteration 8, loss = 0.69233963\n",
      "Iteration 9, loss = 0.69244281\n",
      "Iteration 10, loss = 0.69333720\n",
      "Iteration 11, loss = 0.69287933\n",
      "Iteration 12, loss = 0.69271513\n",
      "Iteration 13, loss = 0.69239755\n",
      "Iteration 14, loss = 0.69259362\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.40765523\n",
      "Iteration 2, loss = 0.71906988\n",
      "Iteration 3, loss = 0.69238969\n",
      "Iteration 4, loss = 0.69251403\n",
      "Iteration 5, loss = 0.69213369\n",
      "Iteration 6, loss = 0.69196915\n",
      "Iteration 7, loss = 0.69191251\n",
      "Iteration 8, loss = 0.69193191\n",
      "Iteration 9, loss = 0.69182355\n",
      "Iteration 10, loss = 0.69189134\n",
      "Iteration 11, loss = 0.69250744\n",
      "Iteration 12, loss = 0.68777202\n",
      "Iteration 13, loss = 0.52785036\n",
      "Iteration 14, loss = 0.29869585\n",
      "Iteration 15, loss = 0.18610873\n",
      "Iteration 16, loss = 0.12637029\n",
      "Iteration 17, loss = 0.12992681\n",
      "Iteration 18, loss = 0.13425222\n",
      "Iteration 19, loss = 0.11342748\n",
      "Iteration 20, loss = 0.09386751\n",
      "Iteration 21, loss = 0.09102634\n",
      "Iteration 22, loss = 0.09764019\n",
      "Iteration 23, loss = 0.14706402\n",
      "Iteration 24, loss = 0.10372432\n",
      "Iteration 25, loss = 0.08918068\n",
      "Iteration 26, loss = 0.08652483\n",
      "Iteration 27, loss = 0.08046793\n",
      "Iteration 28, loss = 0.10350365\n",
      "Iteration 29, loss = 0.09216162\n",
      "Iteration 30, loss = 0.08128370\n",
      "Iteration 31, loss = 0.10194264\n",
      "Iteration 32, loss = 0.08060257\n",
      "Iteration 33, loss = 0.07631149\n",
      "Iteration 34, loss = 0.07184202\n",
      "Iteration 35, loss = 0.07587192\n",
      "Iteration 36, loss = 0.07945366\n",
      "Iteration 37, loss = 0.08365515\n",
      "Iteration 38, loss = 0.08744709\n",
      "Iteration 39, loss = 0.08203719\n",
      "Iteration 40, loss = 0.07253112\n",
      "Iteration 41, loss = 0.07346982\n",
      "Iteration 42, loss = 0.07445647\n",
      "Iteration 43, loss = 0.08073988\n",
      "Iteration 44, loss = 0.07877114\n",
      "Iteration 45, loss = 0.09276831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.58761407\n",
      "Iteration 2, loss = 0.64577577\n",
      "Iteration 3, loss = 0.31802610\n",
      "Iteration 4, loss = 0.15542610\n",
      "Iteration 5, loss = 0.10567616\n",
      "Iteration 6, loss = 0.09134469\n",
      "Iteration 7, loss = 0.10160674\n",
      "Iteration 8, loss = 0.12914767\n",
      "Iteration 9, loss = 0.08970928\n",
      "Iteration 10, loss = 0.10688800\n",
      "Iteration 11, loss = 0.11071970\n",
      "Iteration 12, loss = 0.09741545\n",
      "Iteration 13, loss = 0.10397259\n",
      "Iteration 14, loss = 0.09347007\n",
      "Iteration 15, loss = 0.07555269\n",
      "Iteration 16, loss = 0.07873302\n",
      "Iteration 17, loss = 0.08944252\n",
      "Iteration 18, loss = 0.07614178\n",
      "Iteration 19, loss = 0.07781796\n",
      "Iteration 20, loss = 0.07922953\n",
      "Iteration 21, loss = 0.07211535\n",
      "Iteration 22, loss = 0.06692343\n",
      "Iteration 23, loss = 0.05833024\n",
      "Iteration 24, loss = 0.06254901\n",
      "Iteration 25, loss = 0.05534789\n",
      "Iteration 26, loss = 0.04437646\n",
      "Iteration 27, loss = 0.04292297\n",
      "Iteration 28, loss = 0.04097632\n",
      "Iteration 29, loss = 0.03976995\n",
      "Iteration 30, loss = 0.03830859\n",
      "Iteration 31, loss = 0.03568586\n",
      "Iteration 32, loss = 0.03684915\n",
      "Iteration 33, loss = 0.03837941\n",
      "Iteration 34, loss = 0.03488290\n",
      "Iteration 35, loss = 0.03823472\n",
      "Iteration 36, loss = 0.03678215\n",
      "Iteration 37, loss = 0.03105703\n",
      "Iteration 38, loss = 0.02915945\n",
      "Iteration 39, loss = 0.02353247\n",
      "Iteration 40, loss = 0.02193174\n",
      "Iteration 41, loss = 0.02590713\n",
      "Iteration 42, loss = 0.03810101\n",
      "Iteration 43, loss = 0.03763677\n",
      "Iteration 44, loss = 0.03297626\n",
      "Iteration 45, loss = 0.04599458\n",
      "Iteration 46, loss = 0.03632628\n",
      "Iteration 47, loss = 0.02277459\n",
      "Iteration 48, loss = 0.11179472\n",
      "Iteration 49, loss = 0.06638468\n",
      "Iteration 50, loss = 0.03121421\n",
      "Iteration 51, loss = 0.02011682\n",
      "Iteration 52, loss = 0.01578472\n",
      "Iteration 53, loss = 0.01700238\n",
      "Iteration 54, loss = 0.01672301\n",
      "Iteration 55, loss = 0.01172708\n",
      "Iteration 56, loss = 0.01128914\n",
      "Iteration 57, loss = 0.01045413\n",
      "Iteration 58, loss = 0.01057519\n",
      "Iteration 59, loss = 0.01026333\n",
      "Iteration 60, loss = 0.01087399\n",
      "Iteration 61, loss = 0.01471571\n",
      "Iteration 62, loss = 0.01899906\n",
      "Iteration 63, loss = 0.01089574\n",
      "Iteration 64, loss = 0.00990904\n",
      "Iteration 65, loss = 0.01032362\n",
      "Iteration 66, loss = 0.00964881\n",
      "Iteration 67, loss = 0.00828384\n",
      "Iteration 68, loss = 0.00872874\n",
      "Iteration 69, loss = 0.01081234\n",
      "Iteration 70, loss = 0.01924501\n",
      "Iteration 71, loss = 0.01048446\n",
      "Iteration 72, loss = 0.01089957\n",
      "Iteration 73, loss = 0.00918652\n",
      "Iteration 74, loss = 0.01128533\n",
      "Iteration 75, loss = 0.01113102\n",
      "Iteration 76, loss = 0.01344860\n",
      "Iteration 77, loss = 0.00914669\n",
      "Iteration 78, loss = 0.00860056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.54805714\n",
      "Iteration 2, loss = 0.38848085\n",
      "Iteration 3, loss = 0.22712867\n",
      "Iteration 4, loss = 0.16252292\n",
      "Iteration 5, loss = 0.10740716\n",
      "Iteration 6, loss = 0.08519496\n",
      "Iteration 7, loss = 0.07558297\n",
      "Iteration 8, loss = 0.07104330\n",
      "Iteration 9, loss = 0.06324389\n",
      "Iteration 10, loss = 0.07748178\n",
      "Iteration 11, loss = 0.06794034\n",
      "Iteration 12, loss = 0.04579914\n",
      "Iteration 13, loss = 0.03971759\n",
      "Iteration 14, loss = 0.03585671\n",
      "Iteration 15, loss = 0.03001534\n",
      "Iteration 16, loss = 0.02454927\n",
      "Iteration 17, loss = 0.02291686\n",
      "Iteration 18, loss = 0.02351124\n",
      "Iteration 19, loss = 0.02517141\n",
      "Iteration 20, loss = 0.01722520\n",
      "Iteration 21, loss = 0.01818462\n",
      "Iteration 22, loss = 0.03276274\n",
      "Iteration 23, loss = 0.01782600\n",
      "Iteration 24, loss = 0.01745881\n",
      "Iteration 25, loss = 0.02475307\n",
      "Iteration 26, loss = 0.02949444\n",
      "Iteration 27, loss = 0.01330834\n",
      "Iteration 28, loss = 0.01028368\n",
      "Iteration 29, loss = 0.01514048\n",
      "Iteration 30, loss = 0.01252003\n",
      "Iteration 31, loss = 0.01356909\n",
      "Iteration 32, loss = 0.01120002\n",
      "Iteration 33, loss = 0.01274264\n",
      "Iteration 34, loss = 0.00600807\n",
      "Iteration 35, loss = 0.00568907\n",
      "Iteration 36, loss = 0.00452565\n",
      "Iteration 37, loss = 0.00439644\n",
      "Iteration 38, loss = 0.00529566\n",
      "Iteration 39, loss = 0.00478848\n",
      "Iteration 40, loss = 0.00508493\n",
      "Iteration 41, loss = 0.00566337\n",
      "Iteration 42, loss = 0.01288817\n",
      "Iteration 43, loss = 0.04903664\n",
      "Iteration 44, loss = 0.04447871\n",
      "Iteration 45, loss = 0.04305571\n",
      "Iteration 46, loss = 0.14302148\n",
      "Iteration 47, loss = 0.25043095\n",
      "Iteration 48, loss = 0.13275319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50401700\n",
      "Iteration 2, loss = 0.47463502\n",
      "Iteration 3, loss = 0.20151734\n",
      "Iteration 4, loss = 0.16453574\n",
      "Iteration 5, loss = 0.14927341\n",
      "Iteration 6, loss = 0.11861606\n",
      "Iteration 7, loss = 0.08707341\n",
      "Iteration 8, loss = 0.08255190\n",
      "Iteration 9, loss = 0.09806925\n",
      "Iteration 10, loss = 0.09442180\n",
      "Iteration 11, loss = 0.09349801\n",
      "Iteration 12, loss = 0.07657863\n",
      "Iteration 13, loss = 0.06210266\n",
      "Iteration 14, loss = 0.06345450\n",
      "Iteration 15, loss = 0.06008132\n",
      "Iteration 16, loss = 0.05662045\n",
      "Iteration 17, loss = 0.04659883\n",
      "Iteration 18, loss = 0.03981069\n",
      "Iteration 19, loss = 0.03607368\n",
      "Iteration 20, loss = 0.03427273\n",
      "Iteration 21, loss = 0.03011611\n",
      "Iteration 22, loss = 0.03085200\n",
      "Iteration 23, loss = 0.02829027\n",
      "Iteration 24, loss = 0.02564106\n",
      "Iteration 25, loss = 0.02001803\n",
      "Iteration 26, loss = 0.02593893\n",
      "Iteration 27, loss = 0.02574688\n",
      "Iteration 28, loss = 0.02058893\n",
      "Iteration 29, loss = 0.02995011\n",
      "Iteration 30, loss = 0.02238935\n",
      "Iteration 31, loss = 0.02984578\n",
      "Iteration 32, loss = 0.09645604\n",
      "Iteration 33, loss = 0.03487142\n",
      "Iteration 34, loss = 0.03036970\n",
      "Iteration 35, loss = 0.02652170\n",
      "Iteration 36, loss = 0.01674962\n",
      "Iteration 37, loss = 0.01285809\n",
      "Iteration 38, loss = 0.01045609\n",
      "Iteration 39, loss = 0.00799637\n",
      "Iteration 40, loss = 0.00819258\n",
      "Iteration 41, loss = 0.00969116\n",
      "Iteration 42, loss = 0.01627142\n",
      "Iteration 43, loss = 0.01207128\n",
      "Iteration 44, loss = 0.01263817\n",
      "Iteration 45, loss = 0.01636835\n",
      "Iteration 46, loss = 0.02644642\n",
      "Iteration 47, loss = 0.04527516\n",
      "Iteration 48, loss = 0.06589002\n",
      "Iteration 49, loss = 0.04854602\n",
      "Iteration 50, loss = 0.06528886\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.07433766\n",
      "Iteration 2, loss = 0.34116383\n",
      "Iteration 3, loss = 0.18230053\n",
      "Iteration 4, loss = 0.12667883\n",
      "Iteration 5, loss = 0.10535037\n",
      "Iteration 6, loss = 0.10530311\n",
      "Iteration 7, loss = 0.12425286\n",
      "Iteration 8, loss = 0.12688092\n",
      "Iteration 9, loss = 0.10215581\n",
      "Iteration 10, loss = 0.09293468\n",
      "Iteration 11, loss = 0.10040940\n",
      "Iteration 12, loss = 0.08630860\n",
      "Iteration 13, loss = 0.07954675\n",
      "Iteration 14, loss = 0.07001086\n",
      "Iteration 15, loss = 0.07117572\n",
      "Iteration 16, loss = 0.06887723\n",
      "Iteration 17, loss = 0.05687044\n",
      "Iteration 18, loss = 0.05196810\n",
      "Iteration 19, loss = 0.04133866\n",
      "Iteration 20, loss = 0.04327788\n",
      "Iteration 21, loss = 0.03184942\n",
      "Iteration 22, loss = 0.02742523\n",
      "Iteration 23, loss = 0.02104525\n",
      "Iteration 24, loss = 0.01724018\n",
      "Iteration 25, loss = 0.01774814\n",
      "Iteration 26, loss = 0.02240247\n",
      "Iteration 27, loss = 0.02726186\n",
      "Iteration 28, loss = 0.02053636\n",
      "Iteration 29, loss = 0.01160794\n",
      "Iteration 30, loss = 0.01110405\n",
      "Iteration 31, loss = 0.00996044\n",
      "Iteration 32, loss = 0.00632392\n",
      "Iteration 33, loss = 0.00617917\n",
      "Iteration 34, loss = 0.00531915\n",
      "Iteration 35, loss = 0.00422736\n",
      "Iteration 36, loss = 0.00459174\n",
      "Iteration 37, loss = 0.00521895\n",
      "Iteration 38, loss = 0.00433372\n",
      "Iteration 39, loss = 0.00360174\n",
      "Iteration 40, loss = 0.00374647\n",
      "Iteration 41, loss = 0.00362442\n",
      "Iteration 42, loss = 0.00307780\n",
      "Iteration 43, loss = 0.00280154\n",
      "Iteration 44, loss = 0.00293256\n",
      "Iteration 45, loss = 0.00274167\n",
      "Iteration 46, loss = 0.00253532\n",
      "Iteration 47, loss = 0.00254953\n",
      "Iteration 48, loss = 0.00232124\n",
      "Iteration 49, loss = 0.00241015\n",
      "Iteration 50, loss = 0.00225243\n",
      "Iteration 51, loss = 0.00233136\n",
      "Iteration 52, loss = 0.00214226\n",
      "Iteration 53, loss = 0.00198699\n",
      "Iteration 54, loss = 0.00207237\n",
      "Iteration 55, loss = 0.00221105\n",
      "Iteration 56, loss = 0.00194156\n",
      "Iteration 57, loss = 0.00192079\n",
      "Iteration 58, loss = 0.00193545\n",
      "Iteration 59, loss = 0.00192851\n",
      "Iteration 60, loss = 0.00336935\n",
      "Iteration 61, loss = 0.00535075\n",
      "Iteration 62, loss = 0.00273082\n",
      "Iteration 63, loss = 0.00334999\n",
      "Iteration 64, loss = 0.04548488\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.47109996\n",
      "Iteration 2, loss = 0.42970593\n",
      "Iteration 3, loss = 0.21892198\n",
      "Iteration 4, loss = 0.14275175\n",
      "Iteration 5, loss = 0.09890604\n",
      "Iteration 6, loss = 0.08057655\n",
      "Iteration 7, loss = 0.06591932\n",
      "Iteration 8, loss = 0.05390505\n",
      "Iteration 9, loss = 0.04774125\n",
      "Iteration 10, loss = 0.03887067\n",
      "Iteration 11, loss = 0.03389068\n",
      "Iteration 12, loss = 0.03205985\n",
      "Iteration 13, loss = 0.02393373\n",
      "Iteration 14, loss = 0.02238076\n",
      "Iteration 15, loss = 0.01943474\n",
      "Iteration 16, loss = 0.01637002\n",
      "Iteration 17, loss = 0.02733156\n",
      "Iteration 18, loss = 0.03223494\n",
      "Iteration 19, loss = 0.02518207\n",
      "Iteration 20, loss = 0.01234094\n",
      "Iteration 21, loss = 0.01269505\n",
      "Iteration 22, loss = 0.00704524\n",
      "Iteration 23, loss = 0.00793252\n",
      "Iteration 24, loss = 0.00523818\n",
      "Iteration 25, loss = 0.00395647\n",
      "Iteration 26, loss = 0.00481101\n",
      "Iteration 27, loss = 0.00565755\n",
      "Iteration 28, loss = 0.00415028\n",
      "Iteration 29, loss = 0.00387248\n",
      "Iteration 30, loss = 0.00323377\n",
      "Iteration 31, loss = 0.00259188\n",
      "Iteration 32, loss = 0.00243394\n",
      "Iteration 33, loss = 0.00223053\n",
      "Iteration 34, loss = 0.00198193\n",
      "Iteration 35, loss = 0.00198144\n",
      "Iteration 36, loss = 0.00231528\n",
      "Iteration 37, loss = 0.00214760\n",
      "Iteration 38, loss = 0.00215482\n",
      "Iteration 39, loss = 0.00197336\n",
      "Iteration 40, loss = 0.00169710\n",
      "Iteration 41, loss = 0.00169540\n",
      "Iteration 42, loss = 0.00141871\n",
      "Iteration 43, loss = 0.00145391\n",
      "Iteration 44, loss = 0.00144507\n",
      "Iteration 45, loss = 0.00134257\n",
      "Iteration 46, loss = 0.00123204\n",
      "Iteration 47, loss = 0.00118204\n",
      "Iteration 48, loss = 0.00121382\n",
      "Iteration 49, loss = 0.00107537\n",
      "Iteration 50, loss = 0.00108828\n",
      "Iteration 51, loss = 0.00105725\n",
      "Iteration 52, loss = 0.00102607\n",
      "Iteration 53, loss = 0.00100774\n",
      "Iteration 54, loss = 0.00098193\n",
      "Iteration 55, loss = 0.00098459\n",
      "Iteration 56, loss = 0.00096197\n",
      "Iteration 57, loss = 0.00094679\n",
      "Iteration 58, loss = 0.00092464\n",
      "Iteration 59, loss = 0.00089878\n",
      "Iteration 60, loss = 0.00087566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.78422749\n",
      "Iteration 2, loss = 0.61415448\n",
      "Iteration 3, loss = 0.40524693\n",
      "Iteration 4, loss = 0.23638139\n",
      "Iteration 5, loss = 0.16538155\n",
      "Iteration 6, loss = 0.15465398\n",
      "Iteration 7, loss = 0.11912721\n",
      "Iteration 8, loss = 0.13085873\n",
      "Iteration 9, loss = 0.10510128\n",
      "Iteration 10, loss = 0.08348915\n",
      "Iteration 11, loss = 0.08187329\n",
      "Iteration 12, loss = 0.07096367\n",
      "Iteration 13, loss = 0.06330772\n",
      "Iteration 14, loss = 0.06681774\n",
      "Iteration 15, loss = 0.05506397\n",
      "Iteration 16, loss = 0.05018584\n",
      "Iteration 17, loss = 0.04867095\n",
      "Iteration 18, loss = 0.04444746\n",
      "Iteration 19, loss = 0.04506226\n",
      "Iteration 20, loss = 0.05069954\n",
      "Iteration 21, loss = 0.03620993\n",
      "Iteration 22, loss = 0.03919004\n",
      "Iteration 23, loss = 0.03007296\n",
      "Iteration 24, loss = 0.04025219\n",
      "Iteration 25, loss = 0.03363684\n",
      "Iteration 26, loss = 0.01993871\n",
      "Iteration 27, loss = 0.01705043\n",
      "Iteration 28, loss = 0.01729587\n",
      "Iteration 29, loss = 0.01526797\n",
      "Iteration 30, loss = 0.01579187\n",
      "Iteration 31, loss = 0.01314388\n",
      "Iteration 32, loss = 0.01401702\n",
      "Iteration 33, loss = 0.01307431\n",
      "Iteration 34, loss = 0.02043153\n",
      "Iteration 35, loss = 0.01627701\n",
      "Iteration 36, loss = 0.01278739\n",
      "Iteration 37, loss = 0.00823939\n",
      "Iteration 38, loss = 0.00608695\n",
      "Iteration 39, loss = 0.00848101\n",
      "Iteration 40, loss = 0.00515463\n",
      "Iteration 41, loss = 0.00526566\n",
      "Iteration 42, loss = 0.00523592\n",
      "Iteration 43, loss = 0.00763724\n",
      "Iteration 44, loss = 0.00525566\n",
      "Iteration 45, loss = 0.00454203\n",
      "Iteration 46, loss = 0.00340614\n",
      "Iteration 47, loss = 0.00301530\n",
      "Iteration 48, loss = 0.00284376\n",
      "Iteration 49, loss = 0.00266257\n",
      "Iteration 50, loss = 0.00241792\n",
      "Iteration 51, loss = 0.00232825\n",
      "Iteration 52, loss = 0.00247977\n",
      "Iteration 53, loss = 0.00237461\n",
      "Iteration 54, loss = 0.00340368\n",
      "Iteration 55, loss = 0.00368557\n",
      "Iteration 56, loss = 0.00608999\n",
      "Iteration 57, loss = 0.01290370\n",
      "Iteration 58, loss = 0.01939423\n",
      "Iteration 59, loss = 0.01645782\n",
      "Iteration 60, loss = 0.00911605\n",
      "Iteration 61, loss = 0.00858163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27594066\n",
      "Iteration 2, loss = 0.69656971\n",
      "Iteration 3, loss = 0.69321884\n",
      "Iteration 4, loss = 0.69332314\n",
      "Iteration 5, loss = 0.69301873\n",
      "Iteration 6, loss = 0.69299588\n",
      "Iteration 7, loss = 0.69298619\n",
      "Iteration 8, loss = 0.69306582\n",
      "Iteration 9, loss = 0.69293873\n",
      "Iteration 10, loss = 0.69323923\n",
      "Iteration 11, loss = 0.69331049\n",
      "Iteration 12, loss = 0.69326485\n",
      "Iteration 13, loss = 0.69328538\n",
      "Iteration 14, loss = 0.69308292\n",
      "Iteration 15, loss = 0.69401186\n",
      "Iteration 16, loss = 0.69328763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.68393787\n",
      "Iteration 2, loss = 0.43799134\n",
      "Iteration 3, loss = 0.25777778\n",
      "Iteration 4, loss = 0.16166533\n",
      "Iteration 5, loss = 0.11676001\n",
      "Iteration 6, loss = 0.09669792\n",
      "Iteration 7, loss = 0.09271687\n",
      "Iteration 8, loss = 0.09429045\n",
      "Iteration 9, loss = 0.08036439\n",
      "Iteration 10, loss = 0.07947377\n",
      "Iteration 11, loss = 0.08019450\n",
      "Iteration 12, loss = 0.07916011\n",
      "Iteration 13, loss = 0.07849579\n",
      "Iteration 14, loss = 0.07798818\n",
      "Iteration 15, loss = 0.07450188\n",
      "Iteration 16, loss = 0.07725531\n",
      "Iteration 17, loss = 0.08246387\n",
      "Iteration 18, loss = 0.12219959\n",
      "Iteration 19, loss = 0.08693815\n",
      "Iteration 20, loss = 0.07193320\n",
      "Iteration 21, loss = 0.08064634\n",
      "Iteration 22, loss = 0.08879711\n",
      "Iteration 23, loss = 0.09170527\n",
      "Iteration 24, loss = 0.07106355\n",
      "Iteration 25, loss = 0.07422609\n",
      "Iteration 26, loss = 0.07843996\n",
      "Iteration 27, loss = 0.06357420\n",
      "Iteration 28, loss = 0.05832014\n",
      "Iteration 29, loss = 0.05226087\n",
      "Iteration 30, loss = 0.05843109\n",
      "Iteration 31, loss = 0.05198140\n",
      "Iteration 32, loss = 0.04874826\n",
      "Iteration 33, loss = 0.04262945\n",
      "Iteration 34, loss = 0.04640187\n",
      "Iteration 35, loss = 0.04115746\n",
      "Iteration 36, loss = 0.03918797\n",
      "Iteration 37, loss = 0.04156869\n",
      "Iteration 38, loss = 0.04692634\n",
      "Iteration 39, loss = 0.04383779\n",
      "Iteration 40, loss = 0.04357447\n",
      "Iteration 41, loss = 0.05457243\n",
      "Iteration 42, loss = 0.04133144\n",
      "Iteration 43, loss = 0.03385666\n",
      "Iteration 44, loss = 0.03181472\n",
      "Iteration 45, loss = 0.02993974\n",
      "Iteration 46, loss = 0.02970998\n",
      "Iteration 47, loss = 0.02704296\n",
      "Iteration 48, loss = 0.02609476\n",
      "Iteration 49, loss = 0.02439517\n",
      "Iteration 50, loss = 0.02220601\n",
      "Iteration 51, loss = 0.02197850\n",
      "Iteration 52, loss = 0.02086196\n",
      "Iteration 53, loss = 0.02423650\n",
      "Iteration 54, loss = 0.02996733\n",
      "Iteration 55, loss = 0.02102373\n",
      "Iteration 56, loss = 0.01839049\n",
      "Iteration 57, loss = 0.03686438\n",
      "Iteration 58, loss = 0.03644052\n",
      "Iteration 59, loss = 0.03591954\n",
      "Iteration 60, loss = 0.02906949\n",
      "Iteration 61, loss = 0.02613320\n",
      "Iteration 62, loss = 0.02343835\n",
      "Iteration 63, loss = 0.02279177\n",
      "Iteration 64, loss = 0.02234657\n",
      "Iteration 65, loss = 0.02601542\n",
      "Iteration 66, loss = 0.03215695\n",
      "Iteration 67, loss = 0.02793492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17033941\n",
      "Iteration 2, loss = 0.32613042\n",
      "Iteration 3, loss = 0.20009491\n",
      "Iteration 4, loss = 0.12824482\n",
      "Iteration 5, loss = 0.10488854\n",
      "Iteration 6, loss = 0.08422701\n",
      "Iteration 7, loss = 0.07628355\n",
      "Iteration 8, loss = 0.07007250\n",
      "Iteration 9, loss = 0.06524457\n",
      "Iteration 10, loss = 0.07380267\n",
      "Iteration 11, loss = 0.08664481\n",
      "Iteration 12, loss = 0.06237752\n",
      "Iteration 13, loss = 0.05046643\n",
      "Iteration 14, loss = 0.04452988\n",
      "Iteration 15, loss = 0.04509899\n",
      "Iteration 16, loss = 0.04227191\n",
      "Iteration 17, loss = 0.04065774\n",
      "Iteration 18, loss = 0.04859535\n",
      "Iteration 19, loss = 0.06149255\n",
      "Iteration 20, loss = 0.03038977\n",
      "Iteration 21, loss = 0.02861105\n",
      "Iteration 22, loss = 0.01973716\n",
      "Iteration 23, loss = 0.01672630\n",
      "Iteration 24, loss = 0.01634632\n",
      "Iteration 25, loss = 0.01453766\n",
      "Iteration 26, loss = 0.01427672\n",
      "Iteration 27, loss = 0.01279796\n",
      "Iteration 28, loss = 0.01108721\n",
      "Iteration 29, loss = 0.00876577\n",
      "Iteration 30, loss = 0.00787686\n",
      "Iteration 31, loss = 0.00864932\n",
      "Iteration 32, loss = 0.00785664\n",
      "Iteration 33, loss = 0.00740276\n",
      "Iteration 34, loss = 0.00680176\n",
      "Iteration 35, loss = 0.00871668\n",
      "Iteration 36, loss = 0.00755011\n",
      "Iteration 37, loss = 0.00573206\n",
      "Iteration 38, loss = 0.00385116\n",
      "Iteration 39, loss = 0.00360042\n",
      "Iteration 40, loss = 0.00314116\n",
      "Iteration 41, loss = 0.00280207\n",
      "Iteration 42, loss = 0.00281187\n",
      "Iteration 43, loss = 0.00252823\n",
      "Iteration 44, loss = 0.00242578\n",
      "Iteration 45, loss = 0.00260734\n",
      "Iteration 46, loss = 0.00237911\n",
      "Iteration 47, loss = 0.00239085\n",
      "Iteration 48, loss = 0.00275917\n",
      "Iteration 49, loss = 0.00291706\n",
      "Iteration 50, loss = 0.00249234\n",
      "Iteration 51, loss = 0.00241082\n",
      "Iteration 52, loss = 0.00215347\n",
      "Iteration 53, loss = 0.00226716\n",
      "Iteration 54, loss = 0.00179070\n",
      "Iteration 55, loss = 0.00175175\n",
      "Iteration 56, loss = 0.00154473\n",
      "Iteration 57, loss = 0.00152710\n",
      "Iteration 58, loss = 0.00147688\n",
      "Iteration 59, loss = 0.00140670\n",
      "Iteration 60, loss = 0.00143766\n",
      "Iteration 61, loss = 0.00167969\n",
      "Iteration 62, loss = 0.00151285\n",
      "Iteration 63, loss = 0.00131568\n",
      "Iteration 64, loss = 0.00130625\n",
      "Iteration 65, loss = 0.00128893\n",
      "Iteration 66, loss = 0.00124569\n",
      "Iteration 67, loss = 0.00125433\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.57728163\n",
      "Iteration 2, loss = 0.31658111\n",
      "Iteration 3, loss = 0.15324254\n",
      "Iteration 4, loss = 0.11526690\n",
      "Iteration 5, loss = 0.10496697\n",
      "Iteration 6, loss = 0.12176998\n",
      "Iteration 7, loss = 0.11891683\n",
      "Iteration 8, loss = 0.09616824\n",
      "Iteration 9, loss = 0.07921121\n",
      "Iteration 10, loss = 0.07222548\n",
      "Iteration 11, loss = 0.05865468\n",
      "Iteration 12, loss = 0.05580618\n",
      "Iteration 13, loss = 0.05982886\n",
      "Iteration 14, loss = 0.06019620\n",
      "Iteration 15, loss = 0.06373570\n",
      "Iteration 16, loss = 0.05545566\n",
      "Iteration 17, loss = 0.06102524\n",
      "Iteration 18, loss = 0.08432898\n",
      "Iteration 19, loss = 0.05832075\n",
      "Iteration 20, loss = 0.04999327\n",
      "Iteration 21, loss = 0.04225363\n",
      "Iteration 22, loss = 0.03383025\n",
      "Iteration 23, loss = 0.03321397\n",
      "Iteration 24, loss = 0.02930448\n",
      "Iteration 25, loss = 0.02747765\n",
      "Iteration 26, loss = 0.02844852\n",
      "Iteration 27, loss = 0.04811365\n",
      "Iteration 28, loss = 0.05672951\n",
      "Iteration 29, loss = 0.04904706\n",
      "Iteration 30, loss = 0.02793302\n",
      "Iteration 31, loss = 0.03186039\n",
      "Iteration 32, loss = 0.02460395\n",
      "Iteration 33, loss = 0.02474053\n",
      "Iteration 34, loss = 0.02396020\n",
      "Iteration 35, loss = 0.02898259\n",
      "Iteration 36, loss = 0.01918071\n",
      "Iteration 37, loss = 0.01760823\n",
      "Iteration 38, loss = 0.01960510\n",
      "Iteration 39, loss = 0.04296153\n",
      "Iteration 40, loss = 0.04383155\n",
      "Iteration 41, loss = 0.06581631\n",
      "Iteration 42, loss = 0.04541175\n",
      "Iteration 43, loss = 0.03077417\n",
      "Iteration 44, loss = 0.02456038\n",
      "Iteration 45, loss = 0.02102367\n",
      "Iteration 46, loss = 0.02058746\n",
      "Iteration 47, loss = 0.01489647\n",
      "Iteration 48, loss = 0.01598391\n",
      "Iteration 49, loss = 0.01732057\n",
      "Iteration 50, loss = 0.01734690\n",
      "Iteration 51, loss = 0.01266247\n",
      "Iteration 52, loss = 0.01994361\n",
      "Iteration 53, loss = 0.02475225\n",
      "Iteration 54, loss = 0.04248514\n",
      "Iteration 55, loss = 0.09536851\n",
      "Iteration 56, loss = 0.06033623\n",
      "Iteration 57, loss = 0.02397574\n",
      "Iteration 58, loss = 0.01720201\n",
      "Iteration 59, loss = 0.01647914\n",
      "Iteration 60, loss = 0.01637604\n",
      "Iteration 61, loss = 0.02585963\n",
      "Iteration 62, loss = 0.02125259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.68557932\n",
      "Iteration 2, loss = 0.41774054\n",
      "Iteration 3, loss = 0.23490301\n",
      "Iteration 4, loss = 0.14059173\n",
      "Iteration 5, loss = 0.10620872\n",
      "Iteration 6, loss = 0.10212900\n",
      "Iteration 7, loss = 0.08749848\n",
      "Iteration 8, loss = 0.09063463\n",
      "Iteration 9, loss = 0.08314472\n",
      "Iteration 10, loss = 0.08007598\n",
      "Iteration 11, loss = 0.08649351\n",
      "Iteration 12, loss = 0.08565296\n",
      "Iteration 13, loss = 0.09178394\n",
      "Iteration 14, loss = 0.09888306\n",
      "Iteration 15, loss = 0.10751138\n",
      "Iteration 16, loss = 0.07732845\n",
      "Iteration 17, loss = 0.06310365\n",
      "Iteration 18, loss = 0.05200555\n",
      "Iteration 19, loss = 0.04335135\n",
      "Iteration 20, loss = 0.03865156\n",
      "Iteration 21, loss = 0.03751791\n",
      "Iteration 22, loss = 0.03383667\n",
      "Iteration 23, loss = 0.02788396\n",
      "Iteration 24, loss = 0.02411929\n",
      "Iteration 25, loss = 0.02296090\n",
      "Iteration 26, loss = 0.02234345\n",
      "Iteration 27, loss = 0.02489812\n",
      "Iteration 28, loss = 0.02735010\n",
      "Iteration 29, loss = 0.02346440\n",
      "Iteration 30, loss = 0.01767510\n",
      "Iteration 31, loss = 0.01718083\n",
      "Iteration 32, loss = 0.01684270\n",
      "Iteration 33, loss = 0.01690348\n",
      "Iteration 34, loss = 0.01633561\n",
      "Iteration 35, loss = 0.01215857\n",
      "Iteration 36, loss = 0.00912580\n",
      "Iteration 37, loss = 0.00842106\n",
      "Iteration 38, loss = 0.00949703\n",
      "Iteration 39, loss = 0.00871148\n",
      "Iteration 40, loss = 0.00776753\n",
      "Iteration 41, loss = 0.00596576\n",
      "Iteration 42, loss = 0.00482535\n",
      "Iteration 43, loss = 0.00479326\n",
      "Iteration 44, loss = 0.00719367\n",
      "Iteration 45, loss = 0.00645960\n",
      "Iteration 46, loss = 0.00685842\n",
      "Iteration 47, loss = 0.00724967\n",
      "Iteration 48, loss = 0.03888410\n",
      "Iteration 49, loss = 0.05259841\n",
      "Iteration 50, loss = 0.03614513\n",
      "Iteration 51, loss = 0.03189555\n",
      "Iteration 52, loss = 0.01106410\n",
      "Iteration 53, loss = 0.00674251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.71524341\n",
      "Iteration 2, loss = 0.41840177\n",
      "Iteration 3, loss = 0.24927655\n",
      "Iteration 4, loss = 0.16773926\n",
      "Iteration 5, loss = 0.12451556\n",
      "Iteration 6, loss = 0.10079836\n",
      "Iteration 7, loss = 0.08856944\n",
      "Iteration 8, loss = 0.08228616\n",
      "Iteration 9, loss = 0.08560553\n",
      "Iteration 10, loss = 0.09290877\n",
      "Iteration 11, loss = 0.08310497\n",
      "Iteration 12, loss = 0.07307500\n",
      "Iteration 13, loss = 0.07826581\n",
      "Iteration 14, loss = 0.07533044\n",
      "Iteration 15, loss = 0.07203938\n",
      "Iteration 16, loss = 0.06561165\n",
      "Iteration 17, loss = 0.07652760\n",
      "Iteration 18, loss = 0.06501798\n",
      "Iteration 19, loss = 0.07209135\n",
      "Iteration 20, loss = 0.06997574\n",
      "Iteration 21, loss = 0.07371266\n",
      "Iteration 22, loss = 0.08275328\n",
      "Iteration 23, loss = 0.06925302\n",
      "Iteration 24, loss = 0.07873243\n",
      "Iteration 25, loss = 0.08496293\n",
      "Iteration 26, loss = 0.10638109\n",
      "Iteration 27, loss = 0.08539806\n",
      "Iteration 28, loss = 0.07737692\n",
      "Iteration 29, loss = 0.09755648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.66235283\n",
      "Iteration 2, loss = 0.56514646\n",
      "Iteration 3, loss = 0.44821578\n",
      "Iteration 4, loss = 0.28798144\n",
      "Iteration 5, loss = 0.18655343\n",
      "Iteration 6, loss = 0.13960346\n",
      "Iteration 7, loss = 0.11642204\n",
      "Iteration 8, loss = 0.10029372\n",
      "Iteration 9, loss = 0.09518251\n",
      "Iteration 10, loss = 0.08850115\n",
      "Iteration 11, loss = 0.09280657\n",
      "Iteration 12, loss = 0.10223699\n",
      "Iteration 13, loss = 0.09582959\n",
      "Iteration 14, loss = 0.07847798\n",
      "Iteration 15, loss = 0.07590020\n",
      "Iteration 16, loss = 0.06625194\n",
      "Iteration 17, loss = 0.06422969\n",
      "Iteration 18, loss = 0.07168894\n",
      "Iteration 19, loss = 0.06765361\n",
      "Iteration 20, loss = 0.05527099\n",
      "Iteration 21, loss = 0.06333353\n",
      "Iteration 22, loss = 0.05951174\n",
      "Iteration 23, loss = 0.06064909\n",
      "Iteration 24, loss = 0.05329608\n",
      "Iteration 25, loss = 0.05576993\n",
      "Iteration 26, loss = 0.08917976\n",
      "Iteration 27, loss = 0.11179338\n",
      "Iteration 28, loss = 0.07860380\n",
      "Iteration 29, loss = 0.05774273\n",
      "Iteration 30, loss = 0.05797725\n",
      "Iteration 31, loss = 0.04460168\n",
      "Iteration 32, loss = 0.04850225\n",
      "Iteration 33, loss = 0.04303437\n",
      "Iteration 34, loss = 0.03794590\n",
      "Iteration 35, loss = 0.03632262\n",
      "Iteration 36, loss = 0.05182161\n",
      "Iteration 37, loss = 0.03379252\n",
      "Iteration 38, loss = 0.03219616\n",
      "Iteration 39, loss = 0.03147969\n",
      "Iteration 40, loss = 0.03498062\n",
      "Iteration 41, loss = 0.04007978\n",
      "Iteration 42, loss = 0.03317628\n",
      "Iteration 43, loss = 0.03188234\n",
      "Iteration 44, loss = 0.03129942\n",
      "Iteration 45, loss = 0.02783536\n",
      "Iteration 46, loss = 0.02694425\n",
      "Iteration 47, loss = 0.02628869\n",
      "Iteration 48, loss = 0.02875839\n",
      "Iteration 49, loss = 0.03007668\n",
      "Iteration 50, loss = 0.03185544\n",
      "Iteration 51, loss = 0.03270449\n",
      "Iteration 52, loss = 0.02915941\n",
      "Iteration 53, loss = 0.02688174\n",
      "Iteration 54, loss = 0.02742728\n",
      "Iteration 55, loss = 0.04228106\n",
      "Iteration 56, loss = 0.02920336\n",
      "Iteration 57, loss = 0.03558066\n",
      "Iteration 58, loss = 0.06661822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.64995505\n",
      "Iteration 2, loss = 0.70294134\n",
      "Iteration 3, loss = 0.69645578\n",
      "Iteration 4, loss = 0.69330427\n",
      "Iteration 5, loss = 0.69283752\n",
      "Iteration 6, loss = 0.67701359\n",
      "Iteration 7, loss = 0.49788058\n",
      "Iteration 8, loss = 0.25488786\n",
      "Iteration 9, loss = 0.16122615\n",
      "Iteration 10, loss = 0.12922466\n",
      "Iteration 11, loss = 0.11604935\n",
      "Iteration 12, loss = 0.09450773\n",
      "Iteration 13, loss = 0.09587155\n",
      "Iteration 14, loss = 0.10733079\n",
      "Iteration 15, loss = 0.12620022\n",
      "Iteration 16, loss = 0.12896203\n",
      "Iteration 17, loss = 0.09581912\n",
      "Iteration 18, loss = 0.09237754\n",
      "Iteration 19, loss = 0.08734265\n",
      "Iteration 20, loss = 0.09216483\n",
      "Iteration 21, loss = 0.08926794\n",
      "Iteration 22, loss = 0.09688635\n",
      "Iteration 23, loss = 0.09092042\n",
      "Iteration 24, loss = 0.08353359\n",
      "Iteration 25, loss = 0.08315072\n",
      "Iteration 26, loss = 0.07199485\n",
      "Iteration 27, loss = 0.07812256\n",
      "Iteration 28, loss = 0.07683174\n",
      "Iteration 29, loss = 0.08340730\n",
      "Iteration 30, loss = 0.08432728\n",
      "Iteration 31, loss = 0.08680590\n",
      "Iteration 32, loss = 0.08503709\n",
      "Iteration 33, loss = 0.06836356\n",
      "Iteration 34, loss = 0.07732405\n",
      "Iteration 35, loss = 0.08196376\n",
      "Iteration 36, loss = 0.07463180\n",
      "Iteration 37, loss = 0.06853164\n",
      "Iteration 38, loss = 0.06821013\n",
      "Iteration 39, loss = 0.07300988\n",
      "Iteration 40, loss = 0.07118891\n",
      "Iteration 41, loss = 0.06930345\n",
      "Iteration 42, loss = 0.06372513\n",
      "Iteration 43, loss = 0.06641711\n",
      "Iteration 44, loss = 0.07118977\n",
      "Iteration 45, loss = 0.06511511\n",
      "Iteration 46, loss = 0.07707418\n",
      "Iteration 47, loss = 0.08400727\n",
      "Iteration 48, loss = 0.07714429\n",
      "Iteration 49, loss = 0.08034150\n",
      "Iteration 50, loss = 0.07246681\n",
      "Iteration 51, loss = 0.07715418\n",
      "Iteration 52, loss = 0.10540653\n",
      "Iteration 53, loss = 0.09905701\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.49701044\n",
      "Iteration 2, loss = 0.53912575\n",
      "Iteration 3, loss = 0.28761480\n",
      "Iteration 4, loss = 0.15688128\n",
      "Iteration 5, loss = 0.12073685\n",
      "Iteration 6, loss = 0.14904694\n",
      "Iteration 7, loss = 0.10430110\n",
      "Iteration 8, loss = 0.10194583\n",
      "Iteration 9, loss = 0.08593760\n",
      "Iteration 10, loss = 0.08010850\n",
      "Iteration 11, loss = 0.08324840\n",
      "Iteration 12, loss = 0.07227100\n",
      "Iteration 13, loss = 0.07368317\n",
      "Iteration 14, loss = 0.07023583\n",
      "Iteration 15, loss = 0.10803063\n",
      "Iteration 16, loss = 0.08294817\n",
      "Iteration 17, loss = 0.08582081\n",
      "Iteration 18, loss = 0.07617145\n",
      "Iteration 19, loss = 0.07090301\n",
      "Iteration 20, loss = 0.07230724\n",
      "Iteration 21, loss = 0.09285670\n",
      "Iteration 22, loss = 0.09885946\n",
      "Iteration 23, loss = 0.06468950\n",
      "Iteration 24, loss = 0.06073921\n",
      "Iteration 25, loss = 0.06048184\n",
      "Iteration 26, loss = 0.06293206\n",
      "Iteration 27, loss = 0.06065055\n",
      "Iteration 28, loss = 0.07113732\n",
      "Iteration 29, loss = 0.06169874\n",
      "Iteration 30, loss = 0.06127566\n",
      "Iteration 31, loss = 0.05798180\n",
      "Iteration 32, loss = 0.06999892\n",
      "Iteration 33, loss = 0.06938609\n",
      "Iteration 34, loss = 0.06795143\n",
      "Iteration 35, loss = 0.07091256\n",
      "Iteration 36, loss = 0.07844780\n",
      "Iteration 37, loss = 0.07264167\n",
      "Iteration 38, loss = 0.08080682\n",
      "Iteration 39, loss = 0.05890818\n",
      "Iteration 40, loss = 0.05688229\n",
      "Iteration 41, loss = 0.06542324\n",
      "Iteration 42, loss = 0.05771381\n",
      "Iteration 43, loss = 0.05536208\n",
      "Iteration 44, loss = 0.05518806\n",
      "Iteration 45, loss = 0.05861013\n",
      "Iteration 46, loss = 0.07129729\n",
      "Iteration 47, loss = 0.06595119\n",
      "Iteration 48, loss = 0.07079310\n",
      "Iteration 49, loss = 0.07514055\n",
      "Iteration 50, loss = 0.06201837\n",
      "Iteration 51, loss = 0.05348922\n",
      "Iteration 52, loss = 0.05476160\n",
      "Iteration 53, loss = 0.06645569\n",
      "Iteration 54, loss = 0.05562818\n",
      "Iteration 55, loss = 0.06503860\n",
      "Iteration 56, loss = 0.09139748\n",
      "Iteration 57, loss = 0.06163428\n",
      "Iteration 58, loss = 0.05518044\n",
      "Iteration 59, loss = 0.05958645\n",
      "Iteration 60, loss = 0.06250872\n",
      "Iteration 61, loss = 0.06666855\n",
      "Iteration 62, loss = 0.08632631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.68440288\n",
      "Iteration 2, loss = 0.70269580\n",
      "Iteration 3, loss = 0.69559421\n",
      "Iteration 4, loss = 0.69171120\n",
      "Iteration 5, loss = 0.69107885\n",
      "Iteration 6, loss = 0.68282675\n",
      "Iteration 7, loss = 0.55416662\n",
      "Iteration 8, loss = 0.33727817\n",
      "Iteration 9, loss = 0.19023180\n",
      "Iteration 10, loss = 0.12877123\n",
      "Iteration 11, loss = 0.10299107\n",
      "Iteration 12, loss = 0.08990274\n",
      "Iteration 13, loss = 0.08137380\n",
      "Iteration 14, loss = 0.07559637\n",
      "Iteration 15, loss = 0.07652614\n",
      "Iteration 16, loss = 0.06800854\n",
      "Iteration 17, loss = 0.07227584\n",
      "Iteration 18, loss = 0.08377423\n",
      "Iteration 19, loss = 0.07882338\n",
      "Iteration 20, loss = 0.07500147\n",
      "Iteration 21, loss = 0.06404522\n",
      "Iteration 22, loss = 0.05821721\n",
      "Iteration 23, loss = 0.06126363\n",
      "Iteration 24, loss = 0.06699820\n",
      "Iteration 25, loss = 0.06514719\n",
      "Iteration 26, loss = 0.05988216\n",
      "Iteration 27, loss = 0.05959319\n",
      "Iteration 28, loss = 0.05631149\n",
      "Iteration 29, loss = 0.05705760\n",
      "Iteration 30, loss = 0.06088590\n",
      "Iteration 31, loss = 0.05750386\n",
      "Iteration 32, loss = 0.05594944\n",
      "Iteration 33, loss = 0.07203865\n",
      "Iteration 34, loss = 0.05982576\n",
      "Iteration 35, loss = 0.06909022\n",
      "Iteration 36, loss = 0.10681231\n",
      "Iteration 37, loss = 0.10291369\n",
      "Iteration 38, loss = 0.09273524\n",
      "Iteration 39, loss = 0.07326938\n",
      "Iteration 40, loss = 0.06925213\n",
      "Iteration 41, loss = 0.06059087\n",
      "Iteration 42, loss = 0.05802485\n",
      "Iteration 43, loss = 0.05813170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.22260415\n",
      "Iteration 2, loss = 0.36345584\n",
      "Iteration 3, loss = 0.18731036\n",
      "Iteration 4, loss = 0.12260876\n",
      "Iteration 5, loss = 0.09924222\n",
      "Iteration 6, loss = 0.08606292\n",
      "Iteration 7, loss = 0.08062897\n",
      "Iteration 8, loss = 0.07625961\n",
      "Iteration 9, loss = 0.07643600\n",
      "Iteration 10, loss = 0.09203205\n",
      "Iteration 11, loss = 0.08688969\n",
      "Iteration 12, loss = 0.08244580\n",
      "Iteration 13, loss = 0.11084464\n",
      "Iteration 14, loss = 0.08731412\n",
      "Iteration 15, loss = 0.05847952\n",
      "Iteration 16, loss = 0.09566039\n",
      "Iteration 17, loss = 0.06324707\n",
      "Iteration 18, loss = 0.05428055\n",
      "Iteration 19, loss = 0.04502631\n",
      "Iteration 20, loss = 0.03949478\n",
      "Iteration 21, loss = 0.04089303\n",
      "Iteration 22, loss = 0.04283584\n",
      "Iteration 23, loss = 0.04356318\n",
      "Iteration 24, loss = 0.05056312\n",
      "Iteration 25, loss = 0.05963708\n",
      "Iteration 26, loss = 0.05739009\n",
      "Iteration 27, loss = 0.04743578\n",
      "Iteration 28, loss = 0.03886853\n",
      "Iteration 29, loss = 0.03163551\n",
      "Iteration 30, loss = 0.03062098\n",
      "Iteration 31, loss = 0.02765528\n",
      "Iteration 32, loss = 0.02453783\n",
      "Iteration 33, loss = 0.02863368\n",
      "Iteration 34, loss = 0.04297189\n",
      "Iteration 35, loss = 0.06108560\n",
      "Iteration 36, loss = 0.08153259\n",
      "Iteration 37, loss = 0.05363775\n",
      "Iteration 38, loss = 0.02793308\n",
      "Iteration 39, loss = 0.03579090\n",
      "Iteration 40, loss = 0.02790379\n",
      "Iteration 41, loss = 0.02757224\n",
      "Iteration 42, loss = 0.02134359\n",
      "Iteration 43, loss = 0.01700013\n",
      "Iteration 44, loss = 0.01426718\n",
      "Iteration 45, loss = 0.01977600\n",
      "Iteration 46, loss = 0.01453296\n",
      "Iteration 47, loss = 0.01509220\n",
      "Iteration 48, loss = 0.01438668\n",
      "Iteration 49, loss = 0.01119201\n",
      "Iteration 50, loss = 0.01067309\n",
      "Iteration 51, loss = 0.00874443\n",
      "Iteration 52, loss = 0.00903460\n",
      "Iteration 53, loss = 0.01273814\n",
      "Iteration 54, loss = 0.02129919\n",
      "Iteration 55, loss = 0.01413916\n",
      "Iteration 56, loss = 0.02926586\n",
      "Iteration 57, loss = 0.03946532\n",
      "Iteration 58, loss = 0.10565999\n",
      "Iteration 59, loss = 0.06398574\n",
      "Iteration 60, loss = 0.02855933\n",
      "Iteration 61, loss = 0.02565580\n",
      "Iteration 62, loss = 0.03000627\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.21972278\n",
      "Iteration 2, loss = 0.44086033\n",
      "Iteration 3, loss = 0.24272108\n",
      "Iteration 4, loss = 0.15628482\n",
      "Iteration 5, loss = 0.12529138\n",
      "Iteration 6, loss = 0.09716219\n",
      "Iteration 7, loss = 0.08340146\n",
      "Iteration 8, loss = 0.08181571\n",
      "Iteration 9, loss = 0.07292270\n",
      "Iteration 10, loss = 0.08737765\n",
      "Iteration 11, loss = 0.08643222\n",
      "Iteration 12, loss = 0.08145684\n",
      "Iteration 13, loss = 0.07208279\n",
      "Iteration 14, loss = 0.07126261\n",
      "Iteration 15, loss = 0.07032984\n",
      "Iteration 16, loss = 0.08747678\n",
      "Iteration 17, loss = 0.07264554\n",
      "Iteration 18, loss = 0.07630477\n",
      "Iteration 19, loss = 0.09628025\n",
      "Iteration 20, loss = 0.08002486\n",
      "Iteration 21, loss = 0.07131940\n",
      "Iteration 22, loss = 0.06262522\n",
      "Iteration 23, loss = 0.06451033\n",
      "Iteration 24, loss = 0.06365100\n",
      "Iteration 25, loss = 0.07184720\n",
      "Iteration 26, loss = 0.07426508\n",
      "Iteration 27, loss = 0.07318212\n",
      "Iteration 28, loss = 0.08592649\n",
      "Iteration 29, loss = 0.07104405\n",
      "Iteration 30, loss = 0.07030410\n",
      "Iteration 31, loss = 0.06977298\n",
      "Iteration 32, loss = 0.06464734\n",
      "Iteration 33, loss = 0.06126714\n",
      "Iteration 34, loss = 0.05616700\n",
      "Iteration 35, loss = 0.06556460\n",
      "Iteration 36, loss = 0.11327360\n",
      "Iteration 37, loss = 0.07878544\n",
      "Iteration 38, loss = 0.07153104\n",
      "Iteration 39, loss = 0.06773124\n",
      "Iteration 40, loss = 0.07180074\n",
      "Iteration 41, loss = 0.06048715\n",
      "Iteration 42, loss = 0.05873769\n",
      "Iteration 43, loss = 0.06109612\n",
      "Iteration 44, loss = 0.05860516\n",
      "Iteration 45, loss = 0.07464065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.44444809\n",
      "Iteration 2, loss = 0.69501837\n",
      "Iteration 3, loss = 0.69400530\n",
      "Iteration 4, loss = 0.69323733\n",
      "Iteration 5, loss = 0.69309135\n",
      "Iteration 6, loss = 0.69310791\n",
      "Iteration 7, loss = 0.69360993\n",
      "Iteration 8, loss = 0.69314147\n",
      "Iteration 9, loss = 0.69326252\n",
      "Iteration 10, loss = 0.69370359\n",
      "Iteration 11, loss = 0.69337649\n",
      "Iteration 12, loss = 0.69285157\n",
      "Iteration 13, loss = 0.69303028\n",
      "Iteration 14, loss = 0.69345734\n",
      "Iteration 15, loss = 0.69380395\n",
      "Iteration 16, loss = 0.69366149\n",
      "Iteration 17, loss = 0.69391699\n",
      "Iteration 18, loss = 0.69439395\n",
      "Iteration 19, loss = 0.69328142\n",
      "Iteration 20, loss = 0.69356334\n",
      "Iteration 21, loss = 0.69304568\n",
      "Iteration 22, loss = 0.69318583\n",
      "Iteration 23, loss = 0.69346946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.87202921\n",
      "Iteration 2, loss = 0.70458321\n",
      "Iteration 3, loss = 0.69379751\n",
      "Iteration 4, loss = 0.69301654\n",
      "Iteration 5, loss = 0.69216076\n",
      "Iteration 6, loss = 0.69256463\n",
      "Iteration 7, loss = 0.69244998\n",
      "Iteration 8, loss = 0.69268590\n",
      "Iteration 9, loss = 0.69347355\n",
      "Iteration 10, loss = 0.69216847\n",
      "Iteration 11, loss = 0.69220638\n",
      "Iteration 12, loss = 0.69253699\n",
      "Iteration 13, loss = 0.69243581\n",
      "Iteration 14, loss = 0.69252064\n",
      "Iteration 15, loss = 0.69210740\n",
      "Iteration 16, loss = 0.69217637\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=MLPClassifier(hidden_layer_sizes=(30,),\n",
       "                                          learning_rate_init=0.15,\n",
       "                                          random_state=0, verbose=1),\n",
       "                  n_estimators=25, random_state=0, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(estimator=MLPClassifier(hidden_layer_sizes=(30,),\n",
       "                                          learning_rate_init=0.15,\n",
       "                                          random_state=0, verbose=1),\n",
       "                  n_estimators=25, random_state=0, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(30,), learning_rate_init=0.15, random_state=0,\n",
       "              verbose=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(30,), learning_rate_init=0.15, random_state=0,\n",
       "              verbose=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(estimator=MLPClassifier(hidden_layer_sizes=(30,),\n",
       "                                          learning_rate_init=0.15,\n",
       "                                          random_state=0, verbose=1),\n",
       "                  n_estimators=25, random_state=0, verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = MLPClassifier(\n",
    "    hidden_layer_sizes=(30,),\n",
    "    learning_rate_init=0.15,\n",
    "    momentum=0.9,\n",
    "    random_state=0,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model = BaggingClassifier(\n",
    "    estimator=estimator,\n",
    "    n_estimators=25,\n",
    "    random_state=0,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984375"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1470425 bytes\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "p = pickle.dumps(model)\n",
    "print(f\"{sys.getsizeof(p)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
