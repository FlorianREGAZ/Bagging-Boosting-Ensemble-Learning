@article{1106.0257,
  Author = {R. Maclin and D. Opitz},
  Title = {Popular Ensemble Methods: An Empirical Study},
  Year = {2011},
  Eprint = {arXiv:1106.0257},
  Howpublished = {Journal Of Artificial Intelligence Research, Volume 11, pages 169-198, 1999},
  Doi = {10.1613/jair.614},
}

@Article{Breiman1996,
  author={Breiman, Leo},
  title={Bagging predictors},
  journal={Machine Learning},
  year={1996},
  month={Aug},
  day={01},
  volume={24},
  number={2},
  pages={123-140},
  abstract={Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  issn={1573-0565},
  doi={10.1007/BF00058655},
  url={https://doi.org/10.1007/BF00058655}
}

@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@Article{Schapire1990,
  author={Schapire, Robert E.},
  title={The strength of weak learnability},
  journal={Machine Learning},
  year={1990},
  month={Jun},
  day={01},
  volume={5},
  number={2},
  pages={197-227},
  abstract={This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class islearnable (orstrongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class isweakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.},
  issn={1573-0565},
  doi={10.1007/BF00116037},
  url={https://doi.org/10.1007/BF00116037}
}

@inproceedings{freund1996experiments,
  title={Experiments with a new boosting algorithm},
  author={Freund, Yoav and Schapire, Robert E and others},
  booktitle={icml},
  volume={96},
  pages={148--156},
  year={1996},
  organization={Citeseer}
}

% Gradient Boosting
@techreport{breiman1997arcing,
  title={Arcing the edge},
  author={Breiman, Leo},
  year={1997},
  institution={Citeseer}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@article{friedman2002stochastic,
  title={Stochastic gradient boosting},
  author={Friedman, Jerome H},
  journal={Computational statistics \& data analysis},
  volume={38},
  number={4},
  pages={367--378},
  year={2002},
  publisher={Elsevier}
}

% Examples in Introduction
@article{breast_cancer_prognosis,
title = {A dynamic gradient boosting machine using genetic optimizer for practical breast cancer prognosis},
journal = {Expert Systems with Applications},
volume = {116},
pages = {340-350},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.08.040},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418305542},
author = {Hongya Lu and Haifeng Wang and Sang Won Yoon},
keywords = {Breast cancer prognosis, Online learning, Gradient boosting, Genetic algorithm, Adaptive linear regression},
abstract = {This research proposes a novel genetic algorithm-based online gradient boosting (GAOGB) model for incremental breast cancer (BC) prognosis. The development of clinical information collection technologies has brought in increasingly large amounts of stream data for BC research. Traditional batch learning models have shown limitations in: (1) real-time prognosis accuracy from losing the information of incremental changes of a patient’s pathological condition by time; (2) high redundancy due to the time required to retrain models every time new data are received. Online boosting is an efficient technique for learning from data streams. However, difficulties in parameter assignment and the lack of adaptiveness for batch learning base learners can degrade the performances of typical online boosting algorithms. The main objective of this research is to propose an incremental learning model for BC survivability prediction. To render a boosting algorithm with superiority in global optimal parameters, the genetic algorithm (GA) is integrated to an online gradient boosting scenario at the parameter selection phase, enabling real-time optimization. To enhance adaptiveness, an adaptive linear regressor is adopted as the base learner with minimal computational efforts, and updated in symphony with the online boosting model. The proposed GAOGB model is comprehensively evaluated on the U.S. National Cancer Institute’s Surveillance, Epidemiology, and End Results (SEER) program breast cancer dataset in terms of accuracy, area under the curve (AUC), sensitivity, specificity, retraining time, and variation at each iteration. Experimental results show that the proposed GAOGB model achieves statistically outstanding online learning effectiveness. With a highest 28% improvement on testing accuracy over its base learners, outperforming current state-of-art online learning methods, and approximating batch learning boosting algorithms, the GAOGB algorithm validates the impact of parameter, adaptiveness and convergence in devising practical online learning algorithms. The proposed GAOGB model demonstrates potential for practical incremental breast cancer prognosis, promising a combination of training effectiveness and efficiency.}
}
@article{diabetes_classification,
author = {Nai-arun, Nongyao and Sittidech, Punnee},
year = {2014},
month = {05},
pages = {1427-1431},
title = {Ensemble Learning Model for Diabetes Classification},
volume = {931-932},
journal = {Advanced Materials Research},
doi = {10.4028/www.scientific.net/AMR.931-932.1427}
}

@article{energy_forecasting,
title = {Forecasting mid-long term electric energy consumption through bagging ARIMA and exponential smoothing methods},
journal = {Energy},
volume = {144},
pages = {776-788},
year = {2018},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2017.12.049},
url = {https://www.sciencedirect.com/science/article/pii/S0360544217320820},
author = {Erick Meira {de Oliveira} and Fernando Luiz {Cyrino Oliveira}},
keywords = {Electricity consumption, Forecasting, Bagging},
abstract = {In the last decades, the world's energy consumption has increased rapidly due to fundamental changes in the industry and economy. In such terms, accurate demand forecasts are imperative for decision makers to develop an optimal strategy that includes not only risk reduction, but also the betterment of the economy and society as a whole. This paper expands the fields of application of combined Bootstrap aggregating (Bagging) and forecasting methods to the electric energy sector, a novelty in literature, in order to obtain more accurate demand forecasts. A comparative out-of-sample analysis is conducted using monthly electric energy consumption time series from different countries. The results show that the proposed methodologies substantially improve the forecast accuracy of the demand for energy end-use services in both developed and developing countries. Findings and policy implications are further discussed.}
}


@InProceedings{page_ranking,
  title = 	 {Web-Search Ranking with Initialized Gradient Boosted Regression Trees},
  author = 	 {Mohan, Ananth and Chen, Zheng and Weinberger, Kilian},
  booktitle = 	 {Proceedings of the Learning to Rank Challenge},
  pages = 	 {77--89},
  year = 	 {2011},
  editor = 	 {Chapelle, Olivier and Chang, Yi and Liu, Tie-Yan},
  volume = 	 {14},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Haifa, Israel},
  month = 	 {25 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v14/mohan11a/mohan11a.pdf},
  url = 	 {https://proceedings.mlr.press/v14/mohan11a.html},
  abstract = 	 {In May 2010 Yahoo! Inc. hosted the Learning to Rank Challenge. This paper summarizes the approach by the highly placed team Washington University in St. Louis. We investigate Random Forests (RF) as a low-cost alternative algorithm to Gradient Boosted Regression Trees (GBRT) (the de facto standard of web-search ranking). We demonstrate that it yields surprisingly accurate ranking results – comparable to or better than GBRT. We combine the two algorithms by first learning a ranking function with RF and using it as initialization for GBRT. We refer to this setting as iGBRT. Following a recent discussion byÂ&nbsp;?, we show that the results of iGBRT can be improved upon even further when the web-search ranking task is cast as classification instead of regression. We provide an upper bound of the Expected Reciprocal RankÂ&nbsp;(?) in terms of classification error and demonstrate that iGBRT outperforms GBRT and RF on the Microsoft Learning to Rank and Yahoo Ranking Competition data sets with surprising consistency.}
}


% Datasets
@misc{breast_cancer_wisconsin,
  author       = {Wolberg, William and Mangasarian, Olvi and Street, Nick and Street, W.},
  title        = {Breast Cancer Wisconsin (Diagnostic)},
  year         = {1995},
  howpublished = {UCI Machine Learning Repository},
  note         = {DOI: https://doi.org/10.24432/C5DW2B}
}

@misc{heart_disease_cleveland,
  author       = {Janosi, Andras and Steinbrunn, William and Pfisterer, Matthias and Detrano, Robert},
  title        = {Heart Disease},
  year         = {1988},
  howpublished = {UCI Machine Learning Repository},
  note         = {DOI: https://doi.org/10.24432/C52P4X}
}

@misc{bike_sharing,
  author       = {Fanaee-T,Hadi},
  title        = {Bike Sharing Dataset},
  year         = {2013},
  howpublished = {UCI Machine Learning Repository},
  note         = {DOI: https://doi.org/10.24432/C5W894}
}