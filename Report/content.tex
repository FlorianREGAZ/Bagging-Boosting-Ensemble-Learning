\section{Introduction}
Mandatory. Questions like: What is the topic of this work, what's the broader context (topic of the proseminar), why is it relevant?

%\begin{itemize}
%    \item History of ensemble learning + papers of people "inventing" it
%    \item Goal of the report
%    \item learning more about bagging + boosting
%    \item get to know most popular types of both methods
%    \item learn how to practically use them
%    \item when to use which technique
%    \item 
%\end{itemize}
% ---------------------------------------------------------------------------- %
\section{Ensemble Learning}

Ensemble learning is an advanced machine learning approach that combines the 
strengths of multiple smaller learning algorithms to improve predictive 
performance. 
The concept behind ensemble learning is analogous to the "wisdom of crowds".
Which describes, that a crowd, on average, makes collectively better decisions,
than any single member of it.

Just as a diverse group of people can provide a more accurate collective decision 
than an individual, in ensemble learning, a combination of learning algorithms
often predicts more accurately than an individual learning algorithm.
This approach is based on the principle that a diverse set of learning algorithms
can capture different patterns or trends in the data, leading to more robust and 
accurate predictions.

To be more precise, ensemble methods use multiple smaller learning algorithms,
which specialize in small aspects of the problem. However, by combining these
algorithms, the ensemble often achieves better predictive performance than 
the used algorithm could achieve alone because they complement each others
strengths and weaknesses.

So the goal of ensemble learning is to achieve a better predictive performance.
Nevertheless, it comes at the cost of increased computational resources for 
training as well as prediction and storage.

Overall, there are many different ensemble methods, such as Bagging and Boosting, 
which we will go into more detail in this report. However, there are many more 
like stacking and blending.

% ---------------------------------------------------------------------------- %
\subsection{Bagging}

% Intro Bagging and Traing
Bootstrap Aggregating, commonly known as Bagging, is an ensemble learning method
developed by \citet*{Breiman1996}. The models for the ensemble get trained individually
by using the bootstrapping technique. Bootstrapping involves creating random
subsets of the original training dataset. The subsets are created by drawing 
random data points with replacement and have the same size as the original
training dataset. This means that data points can be chosen more than once and
that some data points might not be in the subset. The models can be trained in
parallel, because of the individual training.


Figure 1 shows how bootstrapping might work on an imaginary dataset. In subset 1
each class is equally distributed. Subset 2 has a focus on the red and orange bubbles.
The subset N has a strong focus on blue, however it doesn't even have a single 
orange bubble. This will probably mean that the model trained on subset N will be very
good at predicting blue, but not very accurate with orange.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\textwidth]{figures/bootstrapping}
    \caption{Creating subsets with bootstrapping.}
\end{figure}

% Prediction Bagging
In the Bagging ensemble, each model makes its prediction independently. Because of
that the predictions of the individual models can be run in parallel, similar to
the training. Once every model in the ensemble has made their prediction, they get
aggregated to form a final ensemble prediction. The method of aggregation depends
on the problem that is being solved by the Bagging ensemble. 
For classification problems, a common method is majority voting. Each model votes 
for a particular class. The class that receives the most votes is chosen for the
final ensemble prediction.
For regression problems, the predictions of the individual models are typically 
averaged.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\textwidth]{figures/bagging_prediction}
    \caption{Bagging prediction example}
\end{figure}


% When to use bagging (Advantages)
% Utilizing Multiple Cores for Parallel Training / Prediction could be added
Bagging can be particularly effective when using base learning algorithms
that have high variance or tend to overfit quickly. By bootstrapping and 
averaging the predictions, the variance gets reduced and potential overfitting
is avoided.
The same goes for unstable learning algorithms, that produce significantly different
results on small changes in the data. That's why Bagging is often used with Decision
Trees as they tend to be unstable and to have high variance.
Additionally, Bagging can be very helpful when dealing with noisy, imbalanced 
datasets or datasets with missing data. The bootstrapping helps to create 
diverse datasets with each class being adequately represented and also averaging
out the noise in combination with aggregation.


% Summary
All in all, Bagging can help to reduce variance, prevent overfitting, and to build
a more resilient, robust, and generalized model.

\subsection{Random Forest}
Random Forest is an ensemble learning method, like Bagging, also developed
by \citet{breiman2001random}. The difference between Bagging and Random Forests
lies in the training of the models within the ensemble. 
First, the base learning algorithm is always a Decision Tree. 
Second, Random Forests utilize a method called Feature Randomness. When 
constructing each tree in a Random Forest, instead of considering all available
features for splitting at each node, a random subset of features is selected.
This randomness reduces the correlation between the individual trees, avoiding
overfitting and increasing robustness and generalization.


% TODO: adden wenn noch Platz
%\subsection{Out-of-bag}

% ---------------------------------------------------------------------------- %
\subsection{Boosting}
Boosting \citep{Schapire1990} is another ensemble learning method like Bagging.
However, unlike Bagging, Boosting trains the models sequentially, where each
model learns from the mistakes of its predecessors.
In the first step, the data subset for the first model gets created and a model 
is trained on it. Initially, the dataset is equally weighted, similar to the initial
setup in Bagging: data points are drawn with replacement until the subset has the 
same size as the original dataset.
In the second step, the performance of the model is evaluated. The weight gets
increased for incorrectly predicted samples and decreased for correctly predicted
samples. 
In the third step, the next model is trained on the dataset with the updated weights.
This weight adjustment makes the next model focus more on the data points that 
previous models predicted incorrectly.
Steps two and three, the process of updating weights and training new models now gets 
repeated until all models are trained.
Different from Bagging, Boosting can't be trained in parallel, because each model
is trained based on the previous model's performance. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\textwidth]{figures/boosting_training}
    \caption{Boosting training example}
\end{figure}

% Boosting prediction
In contrast to the training, each model makes its individual prediction in Boosting.
This means the predictions can be done in parallel.
Unlike Bagging where each model's prediction is given equal weight, Boosting uses
a weighted average approach to combine the individual predictions. This means
each model has its own weight based on the performance in the training phase.
For classification problems, this means that the vote of each model can have
a different influence on the final ensemble prediction.
In the case of regression problems, the final prediction is the weighted average
of the predictions from all models.

% When to use boosting (Advantages)
Boosting is ideal to use when the base learning algorithm suffers from high bias.
This is combated through the way the ensemble is trained, as each model focuses on the
weaknesses of the previous one.
Additionally, Boosting is particularly effective with high-dimensional data, where
the number of features is large. Other models might not be able to capture the 
complexity or more complex models might overfit.
However, Boosting can strike a balance by incrementally building complexity and 
focusing on features that improve the predictive performance.
Furthermore, it's important to note that Boosting works best on datasets with little 
to no outliers. The ensemble strongly focuses on correcting errors. Outliers can 
disproportionately influence the direction of the learning process, leading
to overfitting.


% Boosting summary
To put it in a nutshell, Boosting can help to reduce bias, avoid underfitting,
and can help to build overall precise models. Nevertheless, you have to be aware
of the challenges with Boosting e.g. noisy data.

\subsection{Gradient Boosting}
%\begin{itemize}
%    \item difference to boosting
%\end{itemize}

% TODO: maybe add
% \subsection{Extreme Gradient Boosting}


% ---------------------------------------------------------------------------- %
\section{Examples}
In the examples, we aim to compare various ensemble learning algorithms - Bagging, 
Boosting (AdaBoost - \citet{freund1996experiments}), Random Forest, and Gradient 
Boosting - against Decision Trees across different datasets.
First, we're looking for the optimal depth of the Decision Tree which maximizes the 
precision. This Decision Tree will be used as the base learning algorithm for 
the ensemble learning algorithms.
In order to determine the accuracy of the Decision Tree, the datasets are split
into a validation set (20\%) and the rest (80\%). The rest is used for a 10-fold 
cross-validation, where the accuracy is calculated by averaging the accuracy of 
all folds.
The same method is used to find the best hyperparameters for Bagging, Boosting, 
Random Forest and Gradient Boosting. The learning rate is always 0.1 if 
applicable.
This approach allows us to comprehensively assess each algorithm's effectiveness
and draw meaningful comparisons.


\subsection{Example 1}
In the first example we're using the Breast Cancer Wisconsin (Diagnostic)
dataset by \citet*{breast_cancer_wisconsin}. The dataset originates from
the University of Wisconsin Hospitals. It contains 30 features and has 569
data points. The goal of the dataset is to classify if breast cancer
is benign or malignant.

\input{tables/breast_cancer_wisconsin}


\subsection{Example 2}
In the second example we're using the Heart Disease Cleveland dataset by
\citet*{heart_disease_cleveland}. It contains 13 features and has 303 data 
points. The goal of the dataset is to determine if a patient has a heart
disease.

\input{tables/heart_disease_cleveland}

% ---------------------------------------------------------------------------- %
\section{Summary and conclusion}
Mandatory. Short summary of the most important aspects of the report.
If possible: What are open challenges?

\begin{itemize}
    \item Bagging vs. Boosting - whats the difference?
\end{itemize}
