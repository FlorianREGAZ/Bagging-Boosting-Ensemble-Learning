\section{Introduction}
Mandatory. Questions like: What is the topic of this work, what's the broader context (topic of the proseminar), why is it relevant?

%\begin{itemize}
%    \item History of ensemble learning + papers of people "inventing" it
%    \item Goal of the report
%    \item learning more about bagging + boosting
%    \item get to know most popular types of both methods
%    \item learn how to practically use them
%    \item when to use which technique
%    \item 
%\end{itemize}
% ---------------------------------------------------------------------------- %
\section{Ensemble Learning}

Ensemble learning is an advanced machine learning approach that combines the 
strengths of multiple smaller learning algorithms to improve predictive 
performance. 
The concept behind ensemble learning is analogous to the "wisdom of crowds".
Which describes, that a crowd, on average, makes collectively better decisions,
than any single member of it.

Just as a diverse group of people can provide a more accurate collective decision 
than an individual, in ensemble learning, a combination of learning algorithms
often predicts more accurately than an individual learning algorithm.
This approach is based on the principle that a diverse set of learning algorithms
can capture different patterns or trends in the data, leading to more robust and 
accurate predictions.

To be more precise, ensemble methods use multiple smaller learning algorithms,
which specialize in small aspects of the problem. However, by combining these
algorithms, the ensemble often achieves better predictive performance than 
the used algorithm could achieve alone because they complement each others
strengths and weaknesses.

So the goal of ensemble learning is to achieve a better predictive performance.
Nevertheless, it comes at the cost of increased computational resources for 
training as well as prediction and storage.

Overall, there are many different ensemble methods, such as bagging and boosting, 
which we will go into more detail in this report. However, there are many more 
like stacking and blending.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.5\textwidth]{figures/ensemble_method_prediction}
    \caption{Ensemble learning prediction.}
\end{figure}

% TODO: Make image prettier

% ---------------------------------------------------------------------------- %
\subsection{Bagging}

Bootstrap Aggregating, commonly known as bagging, is an ensemble learning method
developed by Leo Breiman. The models for the ensemble get trained individually
by using the bootstrapping technique. Bootstrapping involves creating random
subsets of the original training dataset. The subsets are created by drawing 
random data point with replacement and have the same size as the original
training dataset. This means that data points can be chosen more than once and
that some data points might not be in the subset. The models can be trained in
parallel, because of the individual training.

% TODO: describe how the selection can work -> duplicates + some completely missing (whats the result) -> get better when combining
% TODO: create figure that shows the shows the process (similar to the one from wikipedia but just the bootstrapping proccess)

In the bagging ensemble each model makes its prediction independently. Because of
that the predictions of the individual models can be ran in parallel, similar to
the training. Once every model in the ensemble has made their prediction, they get
aggregated to form a final ensemble prediction. The method of aggregation depends
on the problem that is being solved by the bagging ensemble. 
For classification problems a common method is majority voting. Each model votes 
for a particular class. The class that receives the most votes is chosen for the
final ensemble prediciton.
For regression problems the predictions of the individual models are typically 
averaged.

% TODO: add figure on how bagging predictions work -> wikipedia image can be used as inspiration

% Intro Bagging

% Training

%\begin{itemize}
%    \item Whats the idea behind it?
%    \item How to train bagging? (+ graphic)
%    \item How does the prediction work? (+ graphic)
%    \item when to use it
%    \item advantages and challenges
%\end{itemize}

% Emphasize Key Concepts: While you correctly describe the bootstrapping process, emphasizing why this is important could add depth. For instance: "This bootstrapping process introduces diversity in the training data for each model, which is crucial for reducing overfitting and improving the robustness of the ensemble."
% Highlight the Importance of Parallel Training: You mention that models can be trained in parallel due to their individual training. Expanding on why this is beneficial could be informative. For instance: "The ability to train models in parallel significantly enhances the efficiency of the bagging method, making it suitable for large datasets and complex models."


\subsection{Random Forest}
%\begin{itemize}
%    \item difference to bagging
%\end{itemize}

% TODO: adden wenn noch Platz - \subsection{Out-of-bag}

\subsection{Boosting}
%\begin{itemize}
%    \item Whats the idea behind it?
%    \item How to train boosting? (+ graphic)
%    \item How does the prediction work? (+ graphic)
%    \item when to use it
%    \item advantages and challenges of using it
%\end{itemize}

\subsection{Gradient Boosting}
%\begin{itemize}
%    \item difference to boosting
%\end{itemize}

\subsection{Extreme Gradient Boosting}
%\begin{itemize}
%    \item difference to gradient boosting
%\end{itemize}


% ---------------------------------------------------------------------------- %
\section{Examples}
\subsection{Example 1}
\subsection{Example 2}

% ---------------------------------------------------------------------------- %
\section{Summary and conclusion}
Mandatory. Short summary of the most important aspects of the report.
If possible: What are open challenges?

\begin{itemize}
    \item Bagging vs. Boosting - whats the difference?
\end{itemize}


%\newpage
%\section{\LaTeX Examples}
%As a help to get started with this template. To be deleted for submission.
%\subsection{Citation examples}
%\citet{campbell:2017} define the stages of information processing in a nervous system as: "sensory input, integration, and motor output". \\
%The stages of information processing in a nervous system are defined as: "sensory input, integration, and motor output" \citep{campbell:2017}. 
%
%\subsection{Table example}
%\input{tables/random_numbers}
%
%\subsection{Figure examples}
%This is a png file, it gets blurry when you zoom in:
%\begin{figure}[htbp]
%    \centering
%    \includegraphics[width=.7\textwidth]{figures/leaky_integration.png}
%    \caption{Symbolic representation of a leaky integrating neuron.}
%    \label{fig:leaky_integration}
%\end{figure}
%
%This is an eps file, it is always sharp:\\
%Notice how the formatting option "[htbp]" allows for the figure to be moved around to page \pageref{fig:activation_function}. Hence, it is best to rather write: The eps file in figure \ref{fig:activation_function} always stays sharp.
%\begin{figure}[htbp]
%    \centering
%    \includegraphics[width=.7\textwidth]{figures/activation_functions}
%    \caption{Shapes of a parametrized tanh activation function.}
%    \label{fig:activation_function}
%\end{figure}
%
%\subsection{Math example}
%The state update of the leaky integrating neuron in figure \ref{fig:leaky_integration} can be formulated as:
%\begin{align}
%    x_i(t+1) &= \lambda_i \cdot \left(W_{i,j} \cdot U_j(t)\right) + (1-\lambda_i) \cdot \theta_i(t)
%    \label{eq:leaky_integration}
%\end{align}
%
%\subsection{Code block example}
%From equation \ref{eq:leaky_integration} the neuron model is implemented using numpy \citep{harris:2020}:
%
%\lstinputlisting[label=py:leaky, language=Python, caption=Python implementation of a single leaky-integrating neuron.]{code/leaky.py}
%
%\subsection{Footnote example}
%The implementation is available on github\footnote{https://github.com/schniewmatz/recurrence}.